---
permalink: /
title: "About Me"
excerpt: "CS Phd Student at UCLA"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, I'm Sid, a second year PhD Student studying Computer Science at UCLA.
I'm advised by Professor [Baharan Mirzasoleiman](http://web.cs.ucla.edu/~baharan/).
I'm currently interested in understanding and exploiting data in multi-modal learning and self-supervised learning (data-efficiency, dataset distillation, data poisoning etc.)

**Looking to recruit undergraduate and graduate students to work with our lab on efficient and robust self-supervised and multi-modal learning.**
[Apply here](https://forms.gle/hfgoVrGxoMABiRRj9) and email me directly if you're interested!

**Pro-bono Office Hours**: I commit 1 hour every week to mentor or provide suggestions for students from underrepresented groups or whoever is in need. If this is you, and you'd like to chat about research, grad school or anything else, please fill out [this form](https://forms.gle/bKrKo3s7Td6Gnkgr7).

In my free time, I like to write ([https://medium.com/@sjoshi804](https://medium.com/@sjoshi804)), cook, play chess and run.

News
======

* June 2023: *[Towards Mitigating Spurious Correlations in the Wild: A Benchmark & a more Realistic Dataset](https://arxiv.org/abs/2306.11957)* preprint on arXiv!
* May 2023: *[Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least](https://sjoshi804.github.io/data-efficient-contrastive-learning/)* accepted to *ICML 2023*!
* May 2023: *[Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression](https://sjoshi804.github.io/icml-cc-fs/)* accepted to *ICML 2023* for an *oral (top 2%)*!
* July 2022: [Low Rank Pruning via Output Perturbation](https://drive.google.com/file/d/1FhuJxrbW554UsMt92WR5B1sCaw8P1odl/view) at *[Sparsity in Neural Networks Workshop](https://www.sparseneural.net)*

Publications
=============
[1] **Siddharth Joshi** and Baharan Mirzasoleiman, *[Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least](https://arxiv.org/abs/2302.09195)*, ICML 2023.

[2] Yihao Xue, **Siddharth Joshi**, Eric Gan, Pin-Yu Chen and Baharan Mirzasoleiman, *Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression*, ICML 2023.

[3] **Siddharth Joshi\***, Yuhan Liu\* and Baharan Mirzasoleiman, *[Low Rank Pruning via Output Perturbation](https://drive.google.com/file/d/1FhuJxrbW554UsMt92WR5B1sCaw8P1odl/view)* at [Sparsity in Neural Networks Workshop](https://www.sparseneural.net) 2022

\* = equal contribution

Preprints
=============

[1] **Siddharth Joshi**, Yu Yang, Yihao Xue, Wenhan Yang and Baharan Mirzasoleiman, *[Towards Mitigating Spurious Correlations in the Wild: A Benchmark & a more Realistic Dataset](https://arxiv.org/abs/2306.11957)*, arXiv.

