---
permalink: /
title: "About Me"
excerpt: "CS Phd Student at UCLA"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, I'm Sid, a second year PhD candidate studying Computer Science at UCLA.
I'm advised by Professor [Baharan Mirzasoleiman](http://web.cs.ucla.edu/~baharan/).
My research focus is on **data-efficiency for learning with limited supervision** i.e. selecting the best small subsets of data for training, to reduce costs without sacrificing accuracy. I aim to develop practically effective and theoretically rigorous approaches to solving these problems.

**Open Office Hours**: In an effort to pay forward all the help I've received in my journey so far in pursuing a career in ML research, I am dedicating 1-2 hours each week for open office hours. This is best suited for relatively junior students (undergraduate/masters) since I'm not very experienced myself :). If you'd like to chat about research, grad school or anything else, please fill out [this form](https://forms.gle/bKrKo3s7Td6Gnkgr7).

In my free time, I like to write ([https://medium.com/@sjoshi804](https://medium.com/@sjoshi804)), read about philosophy and run.

Highlights
======

* **[SAS](https://github.com/sjoshi804/sas-data-efficient-contrastive-learning/tree/master)**: SAS selects subsets of pre-training data to enable data-efficient contrastive SSL (ICML '23). Give it a spin to try out data-efficient SSL!
* **[SpuCo](https://spuco.readthedocs.io/en/latest/)**: SpuCo is a Python package developed to make research on address spurious correlations effortless. Check it out!

News
======

* February 2024: I have successfully *advanced to candidacy*!
* January 2024: *[Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity](https://arxiv.org/abs/2403.12267)* is accepted to AISTATS 2024!
* January 2024: *[Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift](https://yihaoxue.github.io/mmcl-project-page/)* and *[Investigating the Benefits of Projection Head for Representation Learning](https://arxiv.org/abs/2403.11391)* are accepted to ICLR 2024!
* June 2023: *[Towards Mitigating Spurious Correlations in the Wild: A Benchmark & a more Realistic Dataset](https://arxiv.org/abs/2306.11957)* preprint on arXiv!
* May 2023: *[Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least](https://sjoshi804.github.io/data-efficient-contrastive-learning/)* accepted to *ICML 2023*!
* May 2023: *[Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression](https://sjoshi804.github.io/icml-cc-fs/)* accepted to *ICML 2023* for an *oral (top 2%)*!
* July 2022: [Low Rank Pruning via Output Perturbation](https://drive.google.com/file/d/1FhuJxrbW554UsMt92WR5B1sCaw8P1odl/view) at *[Sparsity in Neural Networks Workshop](https://www.sparseneural.net)*

Publications
=============

[1] **Siddharth Joshi**, Arnav Jain, Ali Payani and Baharan Mirzasoleiman, *[Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity](https://arxiv.org/abs/2403.12267)*, **AISTATS 2024**.

[2] Yihao Xue, **Siddharth Joshi**, Dang Nguyen and Baharan Mirzasoleiman, *[Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift](https://arxiv.org/abs/2310.04971)*, **ICLR 2024**.

[3] Yihao Xue, Eric Gan, Jiayi Ni, **Siddharth Joshi** and Baharan Mirzasoleiman, *[Investigating the Benefits of Projection Head for Representation Learning](https://arxiv.org/abs/2403.11391)*, **ICLR 2024**.

[4] **Siddharth Joshi** and Baharan Mirzasoleiman, *[Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least](https://sjoshi804.github.io/data-efficient-contrastive-learning/)*, **ICML 2023**.

[5] Yihao Xue, **Siddharth Joshi**, Eric Gan, Pin-Yu Chen and Baharan Mirzasoleiman, *[Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression](https://sjoshi804.github.io/icml-cc-fs/)*, **ICML 2023 (Oral)**.

[6] **Siddharth Joshi\***, Yuhan Liu\* and Baharan Mirzasoleiman, *[Low Rank Pruning via Output Perturbation](https://drive.google.com/file/d/1FhuJxrbW554UsMt92WR5B1sCaw8P1odl/view)*, **[Sparsity in Neural Networks Workshop](https://www.sparseneural.net) 2022**.

\* = equal contribution

Preprints
=============

[1] **Siddharth Joshi**, Yu Yang, Yihao Xue, Wenhan Yang and Baharan Mirzasoleiman, *[Towards Mitigating Spurious Correlations in the Wild: A Benchmark & a more Realistic Dataset](https://arxiv.org/abs/2306.11957)*, arXiv.
